{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-conv-0.08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se-UcWCyfzs0",
        "outputId": "8c06934a-50fa-42bc-9646-f1447e5d084f"
      },
      "source": [
        "import torch\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor\n",
        "from os.path import expanduser\n",
        "import os\n",
        "\n",
        "dataFolder = \".\"\n",
        "\n",
        "\n",
        "train_ds = torchvision.datasets.MNIST(root=dataFolder, download=True, train=True, transform=ToTensor())\n",
        "test_ds = torchvision.datasets.MNIST(root=dataFolder, download=True, train=False, transform=ToTensor())\n",
        "\n",
        "print(train_ds[0][0].shape)\n",
        "\n",
        "train_dl = DataLoader(dataset=train_ds, batch_size=128, shuffle=True)\n",
        "test_dl = DataLoader(dataset=train_ds, batch_size=128)\n",
        "\n",
        "def convolution(inChannels, outChannels, useMaxPool=False):\n",
        "    conv = nn.Conv2d(inChannels, outChannels, kernel_size=3, padding=1)\n",
        "    bn = nn.BatchNorm2d(num_features=outChannels)\n",
        "    relu = nn.ReLU()\n",
        "\n",
        "    if useMaxPool:\n",
        "        return nn.Sequential(conv, bn, relu, nn.MaxPool2d(2))\n",
        "    else:\n",
        "        return nn.Sequential(conv, bn, relu)\n",
        "\n",
        "class MnistDataset(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistDataset, self).__init__()\n",
        "        self.initLayer = convolution(1, 32, True) # 14x14\n",
        "        self.conv1 = nn.Sequential(convolution(32, 32), convolution(32, 32))\n",
        "        self.middleLayer = convolution(32, 64, True) # 7x7\n",
        "        self.conv2 = nn.Sequential(convolution(64, 64), convolution(64, 64))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.MaxPool2d(7),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.initLayer(x)\n",
        "        out = self.conv1(out) + out\n",
        "        out = self.middleLayer(out)\n",
        "        out = self.conv2(out) + out\n",
        "        return self.classifier(out)\n",
        "\n",
        "\n",
        "numEpoch = 10\n",
        "model = MnistDataset()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=10e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 10e-2, epochs=numEpoch, steps_per_epoch=len(train_dl))\n",
        "\n",
        "\n",
        "\n",
        "def calculateAccuracy(yPredicted, y):\n",
        "    _, yMax = torch.max(yPredicted, dim=1)\n",
        "    return torch.tensor(torch.sum(yMax == y).item() / len(y))\n",
        "\n",
        "\n",
        "for epoch in range(numEpoch):\n",
        "    model.train()\n",
        "    i = 0\n",
        "    for x, y in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        yPredicted = model(x)\n",
        "        loss = cross_entropy(yPredicted, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if i%100 == 0:\n",
        "            print(\"loss=\", loss, \"num=\",i,\"/\",len(train_dl))\n",
        "        i = i + 1\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in test_dl:\n",
        "        yPredicted = model(x)\n",
        "        loss = cross_entropy(yPredicted, y).detach()\n",
        "        accuracy = calculateAccuracy(yPredicted, y)\n",
        "        losses.append(loss)\n",
        "\n",
        "        total += y.size(0)\n",
        "        _, yMax = torch.max(yPredicted, dim=1)\n",
        "        correct += (yMax == y).sum().item()\n",
        "    avgLoss = torch.stack(losses).mean()\n",
        "    avgAccuracy = correct / total * 100\n",
        "    print(\"loss=\", avgLoss, \"acc=\", avgAccuracy,\"%\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "loss= tensor(6.3309, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.3467, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.1699, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.1072, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0730, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0937) acc= 97.14 %\n",
            "loss= tensor(0.1115, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0969, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.1748, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0903, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0366, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0580) acc= 98.24666666666667 %\n",
            "loss= tensor(0.0848, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0825, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0468, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0937, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0223, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0239) acc= 99.29833333333333 %\n",
            "loss= tensor(0.0118, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0468, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0131, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0481, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0215, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.1183) acc= 96.67333333333333 %\n",
            "loss= tensor(0.0344, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0142, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.1042, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0035, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0341, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0252) acc= 99.17333333333333 %\n",
            "loss= tensor(0.0901, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0077, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0103, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0483, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0291, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0149) acc= 99.53166666666667 %\n",
            "loss= tensor(0.0043, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0351, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0026, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0013, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0146, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0123) acc= 99.615 %\n",
            "loss= tensor(0.0053, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0073, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0036, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0031, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0018, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0058) acc= 99.83666666666666 %\n",
            "loss= tensor(0.0234, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0514, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0107, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0008, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0179, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0038) acc= 99.90833333333333 %\n",
            "loss= tensor(0.0052, grad_fn=<NllLossBackward>) num= 0 / 469\n",
            "loss= tensor(0.0346, grad_fn=<NllLossBackward>) num= 100 / 469\n",
            "loss= tensor(0.0062, grad_fn=<NllLossBackward>) num= 200 / 469\n",
            "loss= tensor(0.0014, grad_fn=<NllLossBackward>) num= 300 / 469\n",
            "loss= tensor(0.0042, grad_fn=<NllLossBackward>) num= 400 / 469\n",
            "loss= tensor(0.0033) acc= 99.92166666666667 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}